{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Beta-project - Using AI to classify climbing problems\n",
    "\n",
    "This project uses the Moonboard Database \n",
    "\n",
    "TODO:\n",
    "- use k-fold (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold ?)\n",
    "- make merged graphics\n",
    "- re-test every model and significant variations in order to show the effect of different techniques and their combination\n",
    "- Do a better under-sampling (just removing some samples from majority class, and test training with max 1000 images per class) (add some data augmentation?)"
   ],
   "id": "1ae29cd53e0db30d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# General data consideration\n",
    "\n",
    "We start our project by importing relevant Python libraries, for data science, visualization and machine learning. We'll tackle explainability later."
   ],
   "id": "fca63fa34003f0ad"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download and import data\n",
    "\n",
    "We use an [extracted database](https://github.com/spookykat/MoonBoard/issues/6#issuecomment-1783515787) found on GitHub.\n",
    "\n",
    "We first download everything and import raw data, except the \"problems.json\" that mostly contains duplicates of the Masters 2019 dataset.\n",
    "\n",
    "We include the Mini MoonBoard dataset, as it contains many routes that are still relevant to our problem. The model we'll use will not take into account the size of the route but rather local patterns.\n",
    "\n",
    "We only include columns that will be relevant for our classification problem, i.e. data that influence a route's grade."
   ],
   "id": "c9716f7b999a79cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!wget https://github.com/spookykat/MoonBoard/files/13193317/problems_2023_01_30.zip\n",
    "!unzip problems_2023_01_30 -d problems_2023_01_30"
   ],
   "id": "beab1f4ba2db0023",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = {\n",
    "  \"apiId\": int,\n",
    "  \"name\": str,\n",
    "  \"grade\": 'category',\n",
    "  \"userGrade\": 'category',\n",
    "  \"method\": 'category',\n",
    "  \"holdsetup\": 'category',\n",
    "  \"holdsets\": 'object',\n",
    "  \"moves\": 'object',\n",
    "  \"angle\": int\n",
    "}\n",
    "column_names = list(columns.keys())\n",
    "\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for filename in os.listdir('problems_2023_01_30'):\n",
    "  if filename == 'problems.json':\n",
    "    continue\n",
    "  \n",
    "  with open(os.path.join('problems_2023_01_30', filename), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    local_df = pd.DataFrame(data[\"data\"])\n",
    "    angle = int(filename.rstrip('.json').split()[-1])\n",
    "    local_df['angle'] = angle if angle < 90 else 40\n",
    "    df = pd.concat([df, local_df[column_names]])\n",
    "      \n",
    "df.drop_duplicates(keep='first', subset='apiId', inplace=True)\n",
    "df.set_index('apiId', inplace=True)"
   ],
   "id": "b36f1cc788f2cdbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parse fields",
   "id": "156e95c35272f9c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fields with foreign relations",
   "id": "c27bd23bbd2443b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"holdsetup\"] = df[\"holdsetup\"].map(lambda x: x['apiId'])\n",
    "df[\"holdsets\"] = df[\"holdsets\"].map(lambda sets: [el['apiId'] for el in sets])"
   ],
   "id": "dbbfa556ead88326",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Moves: all holds of the route",
   "id": "7530f4f85246ae75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are three types of holds:\n",
    "\n",
    "- Starter hold: where to put hands at the beginning\n",
    "- Middle hold\n",
    "- End hold: where to put both hands for at least 3 seconds at the end of the route"
   ],
   "id": "cc8fbbbeecb2adb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "WIDTH = 11\n",
    "HEIGHT = 18\n",
    "NUM_HOLD_TYPES = 3\n",
    "\n",
    "MOVES_SHAPE=(WIDTH, HEIGHT, NUM_HOLD_TYPES)\n",
    "\n",
    "def parse_holds(moves):\n",
    "  holds = np.zeros(MOVES_SHAPE, dtype=np.uint8)\n",
    "  for hold in moves:\n",
    "    description = hold['description']\n",
    "    column = ord(description[0].upper()) - ord('A')\n",
    "    row = int(description[1:]) - 1\n",
    "    channel = 0\n",
    "    if hold['isStart']:\n",
    "      channel = 1\n",
    "    if hold['isEnd']:\n",
    "      channel = 2\n",
    "    holds[column, row, channel] = 1\n",
    "  return holds\n",
    "\n",
    "\n",
    "df[\"moves\"] = df[\"moves\"].map(parse_holds)"
   ],
   "id": "db5c5fa0bfc6fa35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Merging \"grade\" and \"userGrade\"\n",
    "\n",
    "When a userGrade is present, it means that enough users rated this route so we assume this is a more objective metric than opener's grade attribution."
   ],
   "id": "16e18c0a7122a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"grade\"] = df[\"userGrade\"].combine_first(df[\"grade\"]).astype('category')\n",
    "df.drop(\"userGrade\", axis=1, inplace=True)"
   ],
   "id": "67bac326cd3250b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Putting the right dtypes",
   "id": "66ea2e3366604c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in df.columns:\n",
    "  df[column] = df[column].astype(columns[column])"
   ],
   "id": "b79fddca3e65b532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Empty data",
   "id": "55d453ea774e5d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "a9d07821cccb196b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is no empty data, but one can uncomment the code below if any shows up.",
   "id": "e9dd18ae8ee479fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df.select_dtypes(include=[int, float]).fillna(df.mean(), inplace=True)\n",
    "# categories = df.select_dtypes(include=['category', 'object'])\n",
    "# categories.fillna(categories.mode().iloc[0])"
   ],
   "id": "2b6bd62aec1b8bf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Visualization and analysis",
   "id": "aceaee28cb32cb59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Overall information",
   "id": "40d0e478ad684a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "1c0911929fc32df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['holdsets'].value_counts()",
   "id": "626654025e49262f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['holdsetup'].value_counts()",
   "id": "9bc74a2aaae4ec22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_grades = list(df['grade'].cat.categories)\n",
    "all_grades"
   ],
   "id": "9209decc7a6b54b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize routes",
   "id": "a18e1b1703df8bbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We plot some of the routes to better visualize their structure. The plot will not be in the same orientation, due to the array structure: the left side is the bottom of the route.",
   "id": "823067d8048fda72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _, route in df.sample(n=6).iterrows():\n",
    "  plt.imshow(route['moves'] * 255)\n",
    "  plt.title(f\"{route['name']} - {route['grade']}\")\n",
    "  plt.show()"
   ],
   "id": "601c5529a13f8566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot the distribution of classes",
   "id": "70d50d66adb3fe90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_category_hist(dataframe, cat):\n",
    "  dataframe[cat].value_counts().sort_index().plot(kind='bar')\n",
    "\n",
    "plot_category_hist(df, \"grade\")"
   ],
   "id": "9684d67d5ea96646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Middle-grade routes are over-represented, with 6B+ routes being clearly omnipresent.",
   "id": "7313e358c9545abf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check the influence of the resolution method",
   "id": "3ce58d0140fcb765"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see another field: \"method\", describing how the climber should physically solve the problem. This can significantly influence the difficulty and feasibility of a route.",
   "id": "df06c590a6fd64a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.groupby('method').size() / len(df)",
   "id": "f7bf416ea0036ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_methods = len(df['method'].dtype.categories)\n",
    "\n",
    "sns.displot(data=df, x='grade', row='method')\n",
    "plt.xticks(rotation=80)"
   ],
   "id": "e684b5493dabc933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "95de9df88f2f48e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "We now want to prepare our data specifically for Machine Learning. We'll apply several well-known techniques, such as encoding (one-hot, rare...) and normalization, in order for our models to learn as much relevant data as possible to generalize well. We'll finish by splitting the data into training and testing sets - the validation set is created directly when training a new model."
   ],
   "id": "abed40ff98d1c10d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Separate features and labels\n",
    "\n",
    "We keep `moves` separate, as it's a special data type that we can tackle in many different ways. We'll see later two views: as a simple sequence, or as images."
   ],
   "id": "a581f1a5fed12765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features = df.drop(columns=['moves', 'grade', 'name'])\n",
    "moves = df['moves']\n",
    "labels = df['grade']"
   ],
   "id": "c411cb03d3fa732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Rare encoding\n",
    "\n",
    "For each categorical feature, we group together categories that appear less than 1% of the time.\n",
    "\n",
    "For instance, the `method` feature has a clear imbalance: thus values `footless`, `footless + kickboard` and `screw ons only` must be gathered together. We can see this group as \"difficult\", as the climber is not allowed to use their feet for these routes."
   ],
   "id": "9934b64e761c523a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rare_threshold = 0.1\n",
    "\n",
    "for col in features.select_dtypes(include=['category']):\n",
    "  value_counts = features[col].value_counts()\n",
    "  rare_values = list(value_counts[value_counts / len(features) < rare_threshold].index)\n",
    "  if len(rare_values) > 1:\n",
    "    features[col] = features[col].cat.add_categories(['Rare'])\n",
    "    features.loc[features[col].isin(rare_values), col] = 'Rare'\n",
    "    features[col] = features[col].cat.remove_unused_categories()"
   ],
   "id": "5881537ae56272cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Before going further, we need to convert all features except 'moves' to numerical ones."
   ],
   "id": "6056383fb331d382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dealing with the `holdsets` feature\n",
    "\n",
    "This field contain arrays of holdsets, so we'll use a one-hot encoding to indicate for each holdset if a route contains it."
   ],
   "id": "fc1403c61a2ac817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: use https://scikit-learn.org/stable/modules/preprocessing_targets.html ?\n",
    "\n",
    "def flatten_array(arr):\n",
    "  result = []\n",
    "  for el in arr:\n",
    "    result.extend(el)\n",
    "  return result\n",
    "\n",
    "all_sets = list(features['holdsets'].value_counts().index)\n",
    "all_sets = flatten_array(all_sets)\n",
    "all_sets = list(set(all_sets))\n",
    "\n",
    "for i in all_sets:\n",
    "  features[f'holdset_{i}'] = False"
   ],
   "id": "352c29076a68070a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def encode_sets(x):\n",
    "  for i in x['holdsets']:\n",
    "    x[f'holdset_{i}'] = True\n",
    "  return x\n",
    "  \n",
    "features = features.apply(encode_sets, axis=1)\n",
    "features.drop(columns=['holdsets'], inplace=True)"
   ],
   "id": "b604fd68ad7e84b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dealing with other features\n",
    "\n",
    "We use a basic one-hot encoding for other features, which is done quickly using pandas."
   ],
   "id": "ebc171ca63fb570a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features['holdsetup'] = features['holdsetup'].astype('category')\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "labels = pd.get_dummies(labels)"
   ],
   "id": "9258fd4127299bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have all the columns, we can get the number of distinct features and labels:",
   "id": "652fd37e467efd44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_labels = len(labels.columns)\n",
    "nb_features = len(features.columns)"
   ],
   "id": "1e395bc2eaa7e141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Normalization\n",
    "\n",
    "We use min-max normalization, as the distribution of numerical inputs (i.e. `angle`) is not normal."
   ],
   "id": "fd57538c352c63a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in features.select_dtypes(include=[int, float]):\n",
    "  features[col] = (features[col] - features[col].min()) / (features[col].max() - features[col].min())"
   ],
   "id": "4e209019bc7c1aea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split into train and test datasets\n",
    "\n",
    "As explained earlier, the validation dataset will be created directly when fitting the model."
   ],
   "id": "9a60e020d4f01ab6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_split = 0.2\n",
    "\n",
    "train_features, test_features, train_moves, test_moves, train_labels, test_labels = train_test_split(\n",
    "  features,\n",
    "  moves,\n",
    "  labels,\n",
    "  test_size=test_split,\n",
    "  stratify=labels,\n",
    ")\n",
    "\n",
    "train_moves = np.stack(train_moves.values)\n",
    "test_moves = np.stack(test_moves.values)"
   ],
   "id": "9d833bf59d737599",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "108800e0b527a1b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Helpers to build, train and analyse models\n",
    "\n",
    "In this part, we define some helpers functions that we are going to reuse for all models : from building a model to analysing results."
   ],
   "id": "50cce4cd9fd4eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Skeleton: build and fit model",
   "id": "4bf5d74ae9a17f30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We first define a function to plot a model, which can be useful to verify the connections made, especially as our models will not be sequential.",
   "id": "bf9f4e4fa99d9db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_model(model):\n",
    "  keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    show_layer_activations=True,\n",
    "    expand_nested=True\n",
    "  )"
   ],
   "id": "d41ef5dc86a1a6d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For all models, we use the Categorical Crossentropy by default, as we face a multi-label classification problem, but where each object only has a unique class.\n",
    "\n",
    "We also use different accuracy metrics not to rely only on the basic accuracy. Indeed, in the reality of climbing, it is okay to be one grade off - in many cases, the climber won't really notice the difference."
   ],
   "id": "438d0fa4d907338f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compile_model(\n",
    "        model=None,\n",
    "        build_function=None,\n",
    "        learning_rate=1e-3,\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=None):\n",
    "\n",
    "    if model is None:\n",
    "        model = build_function()\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics=[]\n",
    "\n",
    "    metrics.extend([\n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.TopKCategoricalAccuracy(k=3, name='accuracy_at_three'),\n",
    "      keras.metrics.TopKCategoricalAccuracy(k=5, name='accuracy_at_five')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model"
   ],
   "id": "241c96e04e5b7140",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we create the function to train models. We use several callbacks to save our models at intermediary and \"best-accuracy\" stages, as well as exporting data for TensorBoard. This will allow us to precisely analyse our results and plot relevant graphs.",
   "id": "164d33b894c81641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(model,\n",
    "                training_features,\n",
    "                training_labels,\n",
    "                epochs=100,\n",
    "                callbacks=None,\n",
    "                early_stopping=None,\n",
    "                validation_split=0.2,\n",
    "                batch_size=64,\n",
    "                class_weight=None,\n",
    "                name='model',\n",
    "                log_dir='logs/fit'\n",
    "                ):\n",
    "\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "        \n",
    "    if early_stopping is not None:\n",
    "        callbacks.append(keras.callbacks.EarlyStopping(monitor='loss', patience=early_stopping))\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks.extend([\n",
    "      keras.callbacks.ModelCheckpoint(f'models/{name}-{date}-{{epoch:02d}}-{{val_loss:.2f}}.keras'),\n",
    "      keras.callbacks.ModelCheckpoint(f'models/{name}-{date}-best.keras', save_best_only=True, monitor='val_loss'),\n",
    "      keras.callbacks.BackupAndRestore(backup_dir=f'/tmp/backup/{name}--{date}'),\n",
    "      keras.callbacks.TensorBoard(log_dir=f'{log_dir}/{name}--{date}', histogram_freq=1)\n",
    "    ])\n",
    "    \n",
    "    model.fit(\n",
    "      training_features,\n",
    "      training_labels,\n",
    "      epochs=epochs,\n",
    "      callbacks=callbacks,\n",
    "      validation_split=validation_split,\n",
    "      batch_size=batch_size,\n",
    "      class_weight=class_weight,\n",
    "    )\n",
    "    return model"
   ],
   "id": "4725754766c3e96c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Skeleton: overall accuracy",
   "id": "4e331983d802c265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "id": "db2e3479b461a348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_prediction(model, moves, features, labels):\n",
    "  probabilities_labels = model.predict([moves, features], verbose=0)\n",
    "  y_true = np.argmax(labels, axis=1)\n",
    "  y_pred = np.argmax(probabilities_labels, axis=1)\n",
    "  predicted = np.zeros_like(probabilities_labels).astype(bool)\n",
    "  predicted[np.arange(probabilities_labels.shape[0]), y_pred] = True\n",
    "  \n",
    "  return predicted, y_true, y_pred\n",
    "  \n",
    "  \n",
    "def load_best_model(\n",
    "        name='model',    \n",
    "  ):\n",
    "    best_model = keras.models.load_model(f'models/{name}-best.keras')\n",
    "    train_predicted, train_y_true, train_y_pred = parse_prediction(best_model, train_moves, train_features, train_labels)\n",
    "    test_predicted, test_y_true, test_y_pred = parse_prediction(best_model, test_moves, test_features, test_labels)\n",
    "    \n",
    "    return best_model, train_predicted, train_y_true, train_y_pred, test_predicted, test_y_true, test_y_pred"
   ],
   "id": "6ed0d91ae9a04365",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_accuracies(model, y_true, y_pred):\n",
    "  metrics = model.evaluate([test_moves, test_features], test_labels, verbose=0)\n",
    "  print(f'Accuracy: {metrics[1] * 100:.2f}%')\n",
    "  print(f'Balanced Accuracy: {balanced_accuracy_score(y_true, y_pred) * 100:.2f}%')\n",
    "  print(f'Accuracy for Top3: {metrics[2] * 100:.2f}%')\n",
    "  print(f'Accuracy for Top5: {metrics[3] * 100:.2f}%')"
   ],
   "id": "d26f1606f27d623b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def confusion_matrix_analysis(y_true, y_pred):\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "  \n",
    "  per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "  for i, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Accuracy for grade {all_grades[i]}: {acc * 100:.2f}%\")\n",
    "  \n",
    "  plt.figure(figsize=(15, 10))\n",
    "  sns.heatmap(cm_normalized,  annot=True, fmt='.2f', cmap='rocket')\n",
    "  plt.xlabel(\"Predicted Labels\")\n",
    "  plt.ylabel(\"True Labels\")\n",
    "  plt.title(\"Normalized Confusion Matrix\")\n",
    "  plt.show()"
   ],
   "id": "c09a281626a834d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Skeleton: one-vs-rest analysis",
   "id": "69a10857f455d659"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_one_vs_rest_plot(xlabel, ylabel):\n",
    "  fig, axs = plt.subplots(nrows=nb_labels, ncols=1, sharex=True)\n",
    "  fig.set_size_inches(6, 4 * nb_labels)\n",
    "  fig.text(0.5, 0.0005, xlabel, ha='center')\n",
    "  fig.text(0.04, 0.5, ylabel, va='center', rotation='vertical')\n",
    "  return fig, axs"
   ],
   "id": "e50ee99a74198e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def one_vs_rest_roc_curve(train_predicted, test_predicted):\n",
    "  fig, axs = create_one_vs_rest_plot('False Positive Rate', 'True Positive Rate')\n",
    "  \n",
    "  for i in range(nb_labels):\n",
    "    train_fpr, train_tpr, _ = roc_curve(train_labels.values[:, i], train_predicted[:, i])\n",
    "    test_fpr, test_tpr, _ = roc_curve(test_labels.values[:, i], test_predicted[:, i])\n",
    "    plt.sca(axs[i])\n",
    "    sns.lineplot(x=train_fpr, y=train_tpr, label='Train', ax=axs[i])\n",
    "    sns.lineplot(x=test_fpr, y=test_tpr, label='Test', ax=axs[i])\n",
    "    plt.ylabel(all_grades[i])\n",
    "    \n",
    "  fig.tight_layout()"
   ],
   "id": "193ffc572d834f4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def one_vs_rest_precision_recall_curve(train_predicted, test_predicted):\n",
    "  fig, axs = create_one_vs_rest_plot('False Positive Rate', 'True Positive Rate')\n",
    "    \n",
    "  for i in range(nb_labels):\n",
    "    train_precision, train_recall, _ = precision_recall_curve(train_labels.values[:, i], train_predicted[:, i])\n",
    "    test_precision, test_recall, _ = precision_recall_curve(test_labels.values[:, i], test_predicted[:, i])\n",
    "    plt.sca(axs[i])\n",
    "    sns.lineplot(x=train_precision, y=train_recall, ax=axs[i])\n",
    "    sns.lineplot(x=test_precision, y=test_recall, ax=axs[i])\n",
    "    plt.ylabel(all_grades[i])\n",
    "      \n",
    "  fig.tight_layout()"
   ],
   "id": "f679e8db9e8c4960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baseline\n",
    "\n",
    "For the baseline, we'll use a simple neural network with only Dense layers. We could even go simpler, but our final goal here is to implement a Deep Learning technique, so we only compare these kinds of algorithms."
   ],
   "id": "3a3ae045fc681051"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_baseline():\n",
    "  moves_inputs = keras.Input(shape=MOVES_SHAPE, name=\"moves\")\n",
    "  features_inputs = keras.Input(shape=(nb_features,), name=\"features\")\n",
    "  \n",
    "  x = keras.layers.Flatten()(moves_inputs)\n",
    "  \n",
    "  x = keras.layers.concatenate([x, features_inputs])\n",
    "  \n",
    "  x = keras.layers.Dense(256, activation='relu')(x)\n",
    "  x = keras.layers.Dense(64, activation='relu')(x)\n",
    "  x = keras.layers.Dropout(0.5)(x)\n",
    "  outputs = keras.layers.Dense(nb_labels, activation='softmax')(x)\n",
    "  \n",
    "  return keras.Model(inputs=[moves_inputs, features_inputs], outputs=outputs)"
   ],
   "id": "dc611e6a1b063551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "baseline_model = compile_model(build_function=create_baseline)",
   "id": "f7c5f0901582cfcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_model(baseline_model)",
   "id": "863058e9410a7289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model training",
   "id": "70c3a8459a8b1a03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(\n",
    "  model=baseline_model,\n",
    "  name='baseline',\n",
    "  training_features=[train_moves, train_features],\n",
    "  training_labels=train_labels,\n",
    "  epochs=50,\n",
    "  early_stopping=3\n",
    ")"
   ],
   "id": "5c5ee49ba06d5c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "baseline_model, train_predicted, train_y_true, train_y_pred, test_predicted, test_y_true, test_y_pred = load_best_model('baseline')",
   "id": "8e190b5a6d0512be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accuracy",
   "id": "e7f2e04e81a0c567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_accuracies(baseline_model, test_y_true, test_y_pred)",
   "id": "7dbf56ae5c455ce7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Our model is overfitting straight away: it's not able to detect any patterns, so it learns the training dataset by heart.",
   "id": "f1ff5afe6c3f1d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Confusion matrix",
   "id": "6d6a83e1f5899cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_matrix_analysis(test_y_true, test_y_pred)",
   "id": "87040a13a6da95c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## One-vs-rest analysis",
   "id": "1170cc8b731225ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "one_vs_rest_roc_curve(train_predicted, test_predicted)",
   "id": "5717fdb835e72872",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For almost every grade, this baseline model doesn't achieve better performance than a random guess. At least, it's not worse.\n",
    "\n",
    "For middle grades (around 6), we get the same insight as with the confusion matrix: the model achieves better performance. Thus the class imbalance has a clear impact and we must apply corrections before going further."
   ],
   "id": "b2206e3e611c3f75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "one_vs_rest_precision_recall_curve(train_predicted, test_predicted)",
   "id": "197cd5a05df853a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Class weights\n",
    "\n",
    "We'll use a Keras feature called class weights, that allow our model to take into account class imbalances and adapt its learning.\n",
    "\n",
    "Here is the distribution of classes in the training dataset:"
   ],
   "id": "7b3614a86eb965a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_labels.sum().plot(kind='bar')",
   "id": "d20083efddd1a920",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate class weights",
   "id": "4b6edba71dee9bf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.arange(nb_labels), y=np.argmax(train_labels, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ],
   "id": "c007817048a209ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train model",
   "id": "2f0a06c0801df84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(\n",
    "  model=compile_model(build_function=create_baseline),\n",
    "  name='baseline_weighted',\n",
    "  training_features=[train_moves, train_features],\n",
    "  training_labels=train_labels,\n",
    "  class_weight=class_weight_dict,\n",
    "  epochs=50,\n",
    ")"
   ],
   "id": "b6a858d3d582474d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This weighted training made the model overfit farther than before: the model started to learn some patterns.",
   "id": "b464485bc7ddb8d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accuracies",
   "id": "afa2d476cf3d9511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "baseline_weighted, train_predicted, train_y_true, train_y_pred, test_predicted, test_y_true, test_y_pred = load_best_model('baseline_weighted')",
   "id": "94c57e0fd209f422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_accuracies(baseline_weighted, test_y_true, test_y_pred)",
   "id": "bfae26ba4e586127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e618938af01f733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Global accuracies are less important, because they are too precise for our climbing problem.\n",
    " \n",
    "Here, the balanced accuracy is better, which is a sign of our model generalizing more and not only skipping rare grades."
   ],
   "id": "1cc40d637a76789d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Confusion matrix",
   "id": "1f4600c61834217d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_matrix_analysis(test_y_true, test_y_pred)",
   "id": "3e13a2b79fa2e1e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Predictions for easier grades are more concentrated on the diagonal, and our model now predicts difficult grades.\n",
    "\n",
    "The new problem here is that our model tends to overestimate difficult routes (lower triangle is more filled in the sub-matrix [10:, 10:])"
   ],
   "id": "884b7a4b01f9e29f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Combine under-sampling and over-sampling\n",
    "\n",
    "We'll try to keep all counts in range `[10000; 20000]` except for extreme classes (8A and further)."
   ],
   "id": "f4c1cbf342aaa4b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_labels.sum().plot(kind='bar')",
   "id": "53691c269d24cc85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Under-sampling",
   "id": "8eb0f09ada81e06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_sampler = RandomUnderSampler(\n",
    "  sampling_strategy={\n",
    "    2: 10_000,\n",
    "    4: 10_000,\n",
    "    5: 10_000,\n",
    "    6: 10_000,\n",
    "    7: 10_000\n",
    "  }\n",
    ")\n",
    "train_features_undersample, train_labels_undersample = under_sampler.fit_resample(train_features.values, train_labels.values)\n",
    "train_moves_undersample, _ = under_sampler.fit_resample(train_moves.reshape(train_moves.shape[0], -1), train_labels.values)"
   ],
   "id": "7dcfdfd5a260bb07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Over-sampling",
   "id": "8c91fe0afb239957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "over_sampler = SMOTE(sampling_strategy={\n",
    "  0: 10_000,\n",
    "  1: 10_000,\n",
    "  3: 10_000,\n",
    "  8: 10_000,\n",
    "  9: 10_000,\n",
    "  10: 10_000,\n",
    "  11: 10_000,\n",
    "  12: 10_000,\n",
    "  13: 5_000,\n",
    "  14: 5_000,\n",
    "  15: 5_000,\n",
    "  16: 5_000,\n",
    "})\n",
    "train_combined_undersample = np.concatenate([train_features_undersample, train_moves_undersample], axis=1)\n",
    "\n",
    "train_combined_resampled, train_labels_resampled = over_sampler.fit_resample(train_combined_undersample, train_labels_undersample)"
   ],
   "id": "fc1f8ab9bf2fd114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_features_resampled = train_combined_resampled[:, :nb_features]\n",
    "train_moves_resampled = train_combined_resampled[:, nb_features:].reshape(-1, *train_moves.shape[1:])"
   ],
   "id": "baa5beb1b18b3dd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_labels_resampled.sum(axis=0)",
   "id": "f3bb352919dc8492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model training",
   "id": "ea462199cb9c7842"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(\n",
    "  model=compile_model(build_function=create_baseline),\n",
    "  name='baseline_resampled',\n",
    "  training_features=[train_moves_resampled, train_features_resampled],\n",
    "  training_labels=train_labels_resampled,\n",
    "  epochs=50,\n",
    ")"
   ],
   "id": "2936e6b8d94f2334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The training accuracy is the best of all, but the validation accuracy is the worst of all... training and validation lost curves keep the same distance for every epoch but with an important gap",
   "id": "5ff40a9c3f36fba7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Treating routes as images: Convolution\n",
    "\n",
    "During Data analysis, we chose to plot our routes as images, with the three separate channels representing the type of hold (start, middle, end). So, if our routes can be interpreted as images, why can't our problem be an image classification problem? That's what we will explore using Convolutional Neural Networks (CNN)."
   ],
   "id": "48405ee025176256"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing multiple configurations\n",
    "\n",
    "Here, we try several different configurations using many techniques :\n",
    "\n",
    "- Conv2D vs SeparableConv2D layers\n",
    "- Number and shape of layers\n",
    "- Padding, strides\n",
    "- Residual connections\n",
    "- Normalization in the middle of the model"
   ],
   "id": "d556c72553c6275c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_convolutional():\n",
    "  shape = [32, 64, 128, 256]\n",
    "  \n",
    "  moves_inputs = keras.Input(shape=MOVES_SHAPE, name=\"moves\")\n",
    "  features_inputs = keras.Input(shape=(nb_features,), name=\"features\")\n",
    "  \n",
    "  x = keras.layers.Dense(64, activation='relu')(features_inputs)\n",
    "  features_outputs = keras.layers.Dense(64, activation='relu')(x)\n",
    "  \n",
    "  # The assumption for using depth wise-separable convolution is channel independence, which is not the case here.\n",
    "  # The three channels are part of the same route, and only indicate the stage (start, middle, end)\n",
    "  y = keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False)(moves_inputs)\n",
    "  \n",
    "  for i, filters in enumerate(shape):\n",
    "    connection = y\n",
    "  \n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    y = keras.layers.SeparableConv2D(filters=filters, kernel_size=3, padding='same', use_bias=False)(y)\n",
    "\n",
    "    y = keras.layers.MaxPool2D(pool_size=2, padding='same')(y)\n",
    "          \n",
    "    # Residual connection fit\n",
    "    connection = keras.layers.Conv2D(filters=filters, kernel_size=1, strides=2, padding='same', use_bias=False)(connection)\n",
    "          \n",
    "    y = keras.layers.add((y, connection))\n",
    "  \n",
    "  moves_outputs = keras.layers.GlobalAveragePooling2D()(y)\n",
    "  \n",
    "  x = keras.layers.concatenate([features_outputs, moves_outputs])\n",
    "  \n",
    "  x = keras.layers.Dense(64, activation='relu')(x)\n",
    "  x = keras.layers.Dropout(0.5)(x)\n",
    "  outputs = keras.layers.Dense(nb_labels, activation='softmax')(x)\n",
    "  \n",
    "  return keras.Model(inputs=[moves_inputs, features_inputs], outputs=outputs)"
   ],
   "id": "8d5d297179d986bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "convolution_model = compile_model(build_function=create_convolutional)",
   "id": "cc1a0d123c13f887",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_model(convolution_model)\n",
    "Image('model.png')"
   ],
   "id": "abb3d70db046b973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_model(\n",
    "  model=convolution_model,\n",
    "  name='convolution',\n",
    "  training_features=[train_moves, train_features],\n",
    "  training_labels=train_labels,\n",
    "  epochs=70\n",
    ")"
   ],
   "id": "27f71572f7421ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "\n",
    "- Overfitting is far away, many epochs can be achieved\n",
    "- Padding don't cause underfitting, but augmenting the end dense network yes\n",
    "- Better performances by normalizing after layers and adding residual connections"
   ],
   "id": "e5e0b3f5493d45ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using a learning rate schedule",
   "id": "b50c10ab678e4040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "convolution_model = compile_model(build_function=create_convolutional, learning_rate=1e-4)\n",
    "train_model(\n",
    "  model=convolution_model,\n",
    "  name='convolution-schedule',\n",
    "  # class_weight=class_weight_dict,\n",
    "  callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor=\"val_loss\",\n",
    "      factor=0.4,\n",
    "      patience=2,\n",
    "      min_lr=1e-6\n",
    "    )\n",
    "  ],\n",
    "  training_features=[train_moves, train_features],\n",
    "  training_labels=train_labels,\n",
    "  epochs=70\n",
    ")"
   ],
   "id": "62d405cc74e935a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "\n",
    "- Slightly better performance when reducing the learning rate to 1e-4\n",
    "- Reduce LR on plateau: only efficient on training validation, as the LR starts to reduce when the model is overfitting\n",
    "- 1e-7 is too low: the loss is not decreasing\n",
    "\n",
    "Conclusion: for this problem, a constant learning rate seems to be the most efficient method"
   ],
   "id": "d7ade34857efd63d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accuracies",
   "id": "556b2f1207ff5766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_convolution, train_predicted, train_y_true, train_y_pred, test_predicted, test_y_true, test_y_pred = load_best_model('convolution-schedule')",
   "id": "8bae32e7b2b3bd68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_accuracies(best_convolution, test_y_true, test_y_pred)",
   "id": "fb062a3e333aeaea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Overall accuracy is the best so far, but balanced accuracy is worse than with class weights. We can add this last technique to further improve our model.",
   "id": "59978c4dba01ce9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explainability\n",
    "\n",
    "We now try to explain the results of our model, by using Shap and visualisation techniques. These last work well with Convolutional Neural Network to plot the filters and get an insight of what's happening."
   ],
   "id": "15b70d8d92ec5967"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Features importance",
   "id": "34b85aab124cb78e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.GradientExplainer(best_convolution, [train_moves, train_features.values])\n",
    "\n",
    "# Shape of shap_values\n",
    "# - First element: (nb_samples, WIDTH, HEIGHT, CHANNELS, nb_outputs)\n",
    "# - Second element: (nb_samples, nb_features, nb_labels)\n",
    "shap_values = explainer.shap_values([test_moves[:4], test_features.values[:4]])"
   ],
   "id": "98a9e1060196c1d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(60, 10))\n",
    "shap.image_plot(\n",
    "  [shap_values[0][:, :, :, :, i] for i in range(nb_labels)],\n",
    "  test_moves[:4] * 255,\n",
    "  labels=np.tile(all_grades, (nb_labels, 1)),\n",
    "  show=False\n",
    ")\n",
    "plt.text(-350, -30, ' '.join(np.vectorize(lambda i: all_grades[i])(np.flip(test_y_pred[:4]))), va='center', rotation='vertical')"
   ],
   "id": "c4d6c70ceb4cb51c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shap.summary_plot(\n",
    "  [shap_values[1][:, :, i] for i in range(nb_labels)],\n",
    "  plot_type='bar',\n",
    "  class_names=all_grades,\n",
    "  feature_names=features.columns\n",
    ")"
   ],
   "id": "8d8c5e4af293f809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Board angle has the most influence on the grade, as it's more difficult to climb a steep route. It has almost an equal importance for all grades, if we take into account class imbalance\n",
    "- Method has the least influence: not fully using feet doesn't make the route more difficult ; it's likely to be due to strong angles on moonboards (25° and 40°), thus the majority of the effort done by the climber is located in the arms and chest"
   ],
   "id": "a7ac904fb2eb8b90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filters visualisation (TODO)",
   "id": "bcc3399052f9d7cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "The conclusion will be written when the project has been completed, so after testing different configurations to reach the best performances, as well as explaining these results. Stay tuned!"
   ],
   "id": "a8744ae6e4891695"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
